{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementa e testa um Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.dim_layers = dim_layers \n",
    "    self.num_layers = len(dim_layers)\n",
    "    self.parameters = self.initialize_parameters()\n",
    "\n",
    "def initialize_parameters(self):\n",
    "    parameters = {}\n",
    "    for i in range(1, self.num_layers):\n",
    "        parameters['W' + str(i)] = np.random.randn(self.dim_layers[i], self.dim_layers[i - 1]) * 0.01\n",
    "        parameters['b' + str(i)] = np.zeros((self.dim_layers[i], 1))\n",
    "    return parameters\n",
    "\n",
    "def relu(self, Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(self, Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "def forward_propagation(self, X):\n",
    "    cache = {}\n",
    "    A = X\n",
    "    for i in range(1, self.num_layers):\n",
    "        Z = np.dot(self.parameters['W' + str(i)], A) + self.parameters['b' + str(i)]\n",
    "        A = self.relu(Z)\n",
    "        cache['Z' + str(i)] = Z\n",
    "        cache['A' + str(i)] = A\n",
    "    cache['A' + str(self.num_layers)] = self.softmax(Z)\n",
    "    return cache    \n",
    "\n",
    "def compute_cost(self, Y, AL):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(AL)) / m\n",
    "    return cost \n",
    "\n",
    "def backward_propagation(self, X, Y, cache):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    dZ = cache['A' + str(self.num_layers)] - Y\n",
    "    for i in range(self.num_layers, 0, -1):\n",
    "        grads['dW' + str(i)] = np.dot(dZ, cache['A' + str(i - 1)].T) / m\n",
    "        grads['db' + str(i)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        if i > 1:\n",
    "            dZ = np.dot(self.parameters['W' + str(i)].T, dZ) * (cache['Z' + str(i - 1)] > 0)\n",
    "    return grads\n",
    "\n",
    "def update_parameters(self, grads, learning_rate):\n",
    "    for i in range(1, self.num_layers):\n",
    "        self.parameters['W' + str(i)] -= learning_rate * grads['dW' + str(i)]\n",
    "        self.parameters['b' + str(i)] -= learning_rate * grads['db' + str(i)]\n",
    "\n",
    "def fit(self, X, Y, learning_rate=0.01, epochs=1000):\n",
    "    for i in range(epochs):\n",
    "        cache = self.forward_propagation(X)\n",
    "        cost = self.compute_cost(Y, cache['A' + str(self.num_layers)])\n",
    "        grads = self.backward_propagation(X, Y, cache)\n",
    "        self.update_parameters(grads, learning_rate)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Custo da iteração {i}: {cost}')\n",
    "\n",
    "def predict(self, X):\n",
    "    cache = self.forward_propagation(X)\n",
    "    return np.argmax(cache['A' + str(self.num_layers)], axis=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
