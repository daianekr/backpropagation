{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementa e testa um Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa as bibliotecas utilizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria a classe MLP que implementa um algoritmo de Multi Layer Perceptron usando Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        \n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2 / layer_sizes[i-1]) for i in range(1, self.num_layers)]\n",
    "        self.biases = [np.zeros((layer_sizes[i], 1)) for i in range(1, self.num_layers)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "            activation_fn = getattr(self, self.activations[i])\n",
    "            activation = activation_fn(z)\n",
    "            activations.append(activation)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, x, y, lr):\n",
    "        m = x.shape[1]\n",
    "        activations = self.forward(x)\n",
    "        deltas = [None] * (self.num_layers - 1)\n",
    "        deltas[-1] = activations[-1] - y\n",
    "        \n",
    "        for i in range(self.num_layers - 2, 0, -1):\n",
    "            delta = np.dot(self.weights[i].T, deltas[i]) * (activations[i] > 0)\n",
    "            deltas[i - 1] = delta\n",
    " \n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights[i] -= lr * np.dot(deltas[i], activations[i].T) / m\n",
    "            self.biases[i] -= lr * np.sum(deltas[i], axis=1, keepdims=True) / m\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(x_train, y_train, lr)\n",
    "            if epoch % 100 == 0:\n",
    "                activations = self.forward(x_train)\n",
    "                loss = -np.mean(np.sum(y_train * np.log(activations[-1]), axis=0))\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        activations = self.forward(x_test)\n",
    "        return np.argmax(activations[-1], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testando com um pequeno dataset de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6580873506718343\n",
      "Epoch 100, Loss: 0.2526273821304806\n",
      "Epoch 200, Loss: 0.15918994087711968\n",
      "Epoch 300, Loss: 0.11268314361324461\n",
      "Epoch 400, Loss: 0.08416234704459528\n",
      "Epoch 500, Loss: 0.06502278719214341\n",
      "Epoch 600, Loss: 0.05165771418437252\n",
      "Epoch 700, Loss: 0.041961416636495746\n",
      "Epoch 800, Loss: 0.03475912921244856\n",
      "Epoch 900, Loss: 0.029303511690233406\n",
      "Predições: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]]).T\n",
    "\n",
    "layer_sizes = [2, 64, 64, 2]  \n",
    "activations = ['relu', 'relu', 'softmax']  \n",
    "\n",
    "mlp = MLP(layer_sizes, activations)\n",
    "mlp.train(X, Y, epochs=1000, lr=0.01)\n",
    "\n",
    "predictions = mlp.predict(X)\n",
    "print(\"Predições:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste Simples usando a MLPClassifier do Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições: [0 1 1 0]\n",
      "Acurácia: 1.0\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 64), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "mlp.fit(X.T, np.argmax(Y, axis=0))  \n",
    "\n",
    "predictions = mlp.predict(X.T)\n",
    "\n",
    "print(\"Predições:\", predictions)\n",
    "accuracy = accuracy_score(np.argmax(Y, axis=0), predictions)\n",
    "print(\"Acurácia:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
