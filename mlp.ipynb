{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementa e testa um Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights_input_hidden = np.random.randn(self.hidden_size, self.input_size)\n",
    "        self.bias_input_hidden = np.zeros((self.hidden_size, 1))\n",
    "        self.weights_hidden_output = np.random.randn(self.output_size, self.hidden_size)\n",
    "        self.bias_hidden_output = np.zeros((self.output_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=0))\n",
    "        return exp_values / np.sum(exp_values, axis=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.hidden_activation = self.sigmoid(np.dot(self.weights_input_hidden, x) + self.bias_input_hidden)\n",
    "        self.output_activation = self.softmax(np.dot(self.weights_hidden_output, self.hidden_activation) + self.bias_hidden_output)\n",
    "        return self.output_activation\n",
    "    \n",
    "    def backward(self, x, y, lr):\n",
    "        m = x.shape[1]\n",
    "        d_output = self.output_activation - y\n",
    "        d_weights_hidden_output = np.dot(d_output, self.hidden_activation.T) / m\n",
    "        d_bias_hidden_output = np.sum(d_output, axis=1, keepdims=True) / m\n",
    "        \n",
    "        d_hidden = np.dot(self.weights_hidden_output.T, d_output) * self.hidden_activation * (1 - self.hidden_activation)\n",
    "        d_weights_input_hidden = np.dot(d_hidden, x.T) / m\n",
    "        d_bias_input_hidden = np.sum(d_hidden, axis=1, keepdims=True) / m\n",
    "        \n",
    "        self.weights_hidden_output -= lr * d_weights_hidden_output\n",
    "        self.bias_hidden_output -= lr * d_bias_hidden_output\n",
    "        self.weights_input_hidden -= lr * d_weights_input_hidden\n",
    "        self.bias_input_hidden -= lr * d_bias_input_hidden\n",
    "        \n",
    "    def train(self, x_train, y_train, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(x_train)\n",
    "            \n",
    "            self.backward(x_train, y_train, lr)\n",
    "            \n",
    "            loss = -np.mean(np.sum(y_train * np.log(output), axis=0))\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        return np.argmax(self.forward(x_test), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3083760529212412\n",
      "Epoch 100, Loss: 0.6951203625355694\n",
      "Epoch 200, Loss: 0.6835624198615804\n",
      "Epoch 300, Loss: 0.6670658718468232\n",
      "Epoch 400, Loss: 0.6443046565556131\n",
      "Epoch 500, Loss: 0.6156910438592876\n",
      "Epoch 600, Loss: 0.5803409571835214\n",
      "Epoch 700, Loss: 0.5367398170161801\n",
      "Epoch 800, Loss: 0.4842218646348029\n",
      "Epoch 900, Loss: 0.4237713833270865\n",
      "Predictions: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]]).T\n",
    "\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=2)\n",
    "mlp.train(X, Y, epochs=1000, lr=0.1)\n",
    "\n",
    "predictions = mlp.predict(X)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        \n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2 / layer_sizes[i-1]) for i in range(1, self.num_layers)]\n",
    "        self.biases = [np.zeros((layer_sizes[i], 1)) for i in range(1, self.num_layers)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "            activation_fn = getattr(self, self.activations[i])\n",
    "            activation = activation_fn(z)\n",
    "            activations.append(activation)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, x, y, lr):\n",
    "        m = x.shape[1]\n",
    "        activations = self.forward(x)\n",
    "        deltas = [None] * (self.num_layers - 1)\n",
    "        deltas[-1] = activations[-1] - y\n",
    "        \n",
    "        for i in range(self.num_layers - 2, 0, -1):\n",
    "            delta = np.dot(self.weights[i].T, deltas[i]) * (activations[i] > 0)\n",
    "            deltas[i - 1] = delta\n",
    " \n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights[i] -= lr * np.dot(deltas[i], activations[i].T) / m\n",
    "            self.biases[i] -= lr * np.sum(deltas[i], axis=1, keepdims=True) / m\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(x_train, y_train, lr)\n",
    "            if epoch % 100 == 0:\n",
    "                activations = self.forward(x_train)\n",
    "                loss = -np.mean(np.sum(y_train * np.log(activations[-1]), axis=0))\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        activations = self.forward(x_test)\n",
    "        return np.argmax(activations[-1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7647937923220266\n",
      "Epoch 100, Loss: 0.30731151853150385\n",
      "Epoch 200, Loss: 0.20147537337070945\n",
      "Epoch 300, Loss: 0.14833541863949468\n",
      "Epoch 400, Loss: 0.11469337604457905\n",
      "Epoch 500, Loss: 0.09134054971012061\n",
      "Epoch 600, Loss: 0.07429183161158502\n",
      "Epoch 700, Loss: 0.061433277429908714\n",
      "Epoch 800, Loss: 0.051562257507589276\n",
      "Epoch 900, Loss: 0.043837245469215295\n",
      "Predições: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]]).T\n",
    "\n",
    "layer_sizes = [2, 64, 64, 2]  \n",
    "activations = ['relu', 'relu', 'softmax']  \n",
    "\n",
    "mlp = MLP(layer_sizes, activations)\n",
    "mlp.train(X, Y, epochs=1000, lr=0.01)\n",
    "\n",
    "predictions = mlp.predict(X)\n",
    "print(\"Predições:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições: [0 1 1 0]\n",
      "Acurácia: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 64), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "mlp.fit(X.T, np.argmax(Y, axis=0))  \n",
    "\n",
    "\n",
    "predictions = mlp.predict(X.T)\n",
    "\n",
    "print(\"Predições:\", predictions)\n",
    "accuracy = accuracy_score(np.argmax(Y, axis=0), predictions)\n",
    "print(\"Acurácia:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
